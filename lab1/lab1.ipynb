{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3eb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 85\n",
      "Unique words: 67\n",
      "\n",
      "Word Frequencies:\n",
      "Counter({'parsing': 3, 'of': 3, 'a': 3, 'or': 3, 'and': 2, 'for': 2, 'it': 2, 'we': 2, 'the': 2, 'many': 2, 'might': 2, 'morphological': 2, 'form': 2, 'to': 2, 'means': 1, 'taking': 1, 'an': 1, 'input': 1, 'producing': 1, 'some': 1, 'sort': 1, 'linguistic': 1, 'structure': 1, 'will': 1, 'use': 1, 'term': 1, 'very': 1, 'broadly': 1, 'throughout': 1, 'this': 1, 'book': 1, 'including': 1, 'kinds': 1, 'structures': 1, 'that': 1, 'be': 1, 'produced': 1, 'syntactic': 1, 'semantic': 1, 'discourse': 1, 'in': 1, 'string': 1, 'tree': 1, 'network': 1, 'stemming': 1, 'applies': 1, 'affixes': 1, 'other': 1, 'than': 1, 'plurals': 1, 'example': 1, 'need': 1, 'take': 1, 'any': 1, 'english': 1, 'verb': 1, 'ending': 1, 'going': 1, 'talking': 1, 'congratulating': 1, 'parse': 1, 'into': 1, 'its': 1, 'verbal': 1, 'stem': 1, 'plus': 1, 'morpheme': 1})\n",
      "\n",
      "Most frequent word: ('parsing', 3)\n",
      "Least frequent word: ('morpheme', 1)\n",
      "Longest word: congratulating\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "paragraph1 = \"\"\"\n",
    "Parsing means taking an input and producing some sort of linguistic structure for it. \n",
    "We will use the term parsing very broadly throughout this book, including many kinds \n",
    "of structures that might be produced; morphological, syntactic, semantic, discourse; in\n",
    "the form of a string, or a tree, or a network. Morphological parsing or stemming applies \n",
    "to many affixes other than plurals; for example we might need to take any English verb\n",
    "form ending in-ing (going, talking, congratulating) and parse it into its verbal stem\n",
    "plus the-ing morpheme.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and clean words\n",
    "words = word_tokenize(paragraph1.lower())\n",
    "words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# Total and unique words\n",
    "total_words = len(words)\n",
    "unique_words = len(set(words))\n",
    "print(\"Total words:\", total_words)\n",
    "print(\"Unique words:\", unique_words)\n",
    "\n",
    "# Word frequencies\n",
    "freq = Counter(words)\n",
    "print(\"\\nWord Frequencies:\")\n",
    "print(freq)\n",
    "\n",
    "# Most frequent word\n",
    "most_freq = freq.most_common(1)[0]\n",
    "print(\"\\nMost frequent word:\", most_freq)\n",
    "\n",
    "# Least frequent word\n",
    "least_freq = freq.most_common()[-1]\n",
    "print(\"Least frequent word:\", least_freq)\n",
    "\n",
    "# Longest word\n",
    "longest_word = max(words, key=len)\n",
    "print(\"Longest word:\", longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a841a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 65\n",
      "Unique words: 50\n",
      "\n",
      "Word Frequencies:\n",
      "Counter({'et': 5, 'ipsum': 2, 'dolor': 2, 'quia': 2, 'ea': 2, 'vero': 2, 'rem': 2, 'ut': 2, 'dolorem': 2, 'eos': 2, 'dolores': 2, 'aut': 2, 'lorem': 1, 'sit': 1, 'amet': 1, 'voluptas': 1, 'deleniti': 1, 'delectus': 1, 'obcaecati': 1, 'perferendis': 1, 'veniam': 1, 'eveniet': 1, 'unde': 1, 'internos': 1, 'impedit': 1, 'dicta': 1, 'fuga': 1, 'error': 1, 'facere': 1, 'eius': 1, 'laboriosam': 1, 'ex': 1, 'debitis': 1, 'provident': 1, 'id': 1, 'repudiandae': 1, 'pariatur': 1, 'vel': 1, 'dolore': 1, 'voluptatum': 1, 'ad': 1, 'quis': 1, 'quas': 1, 'non': 1, 'possimus': 1, 'cupiditate': 1, 'cumque': 1, 'sint': 1, 'voluptate': 1, 'similique': 1})\n",
      "\n",
      "Most frequent word: ('et', 5)\n",
      "Least frequent word: ('similique', 1)\n",
      "Longest word: perferendis\n"
     ]
    }
   ],
   "source": [
    "paragraph2 = \"\"\"\n",
    "Lorem ipsum dolor sit amet. Et quia voluptas et deleniti delectus ea obcaecati perferendis et veniam eveniet. Ea vero unde rem internos impedit et dicta fuga ut dolorem error et facere eius eos laboriosam vero. Ex debitis provident id repudiandae pariatur eos quia dolor vel dolore voluptatum. Ad Quis quas non dolores dolorem aut possimus cupiditate rem cumque ipsum ut sint voluptate aut dolores similique.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and clean words\n",
    "words = word_tokenize(paragraph2.lower())\n",
    "words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# Total and unique words\n",
    "total_words = len(words)\n",
    "unique_words = len(set(words))\n",
    "print(\"Total words:\", total_words)\n",
    "print(\"Unique words:\", unique_words)\n",
    "\n",
    "# Word frequencies\n",
    "freq = Counter(words)\n",
    "print(\"\\nWord Frequencies:\")\n",
    "print(freq)\n",
    "\n",
    "# Most frequent word\n",
    "most_freq = freq.most_common(1)[0]\n",
    "print(\"\\nMost frequent word:\", most_freq)\n",
    "\n",
    "# Least frequent word\n",
    "least_freq = freq.most_common()[-1]\n",
    "print(\"Least frequent word:\", least_freq)\n",
    "\n",
    "# Longest word\n",
    "longest_word = max(words, key=len)\n",
    "print(\"Longest word:\", longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22867eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1 (a) - All alphabetic strings\n",
      "Matches: ['this', 'is']\n",
      "\n",
      "2.1 (b) - Lowercase strings ending in 'b'\n",
      "Matches: ['cab', 'grab', 'slab']\n",
      "\n",
      "2.1 (c) - Two consecutive repeated words\n",
      "Matches: ['the']\n",
      "\n",
      "2.1 (d) - Each 'a' is immediately preceded and followed by 'b'\n",
      "'babbbbab': True\n",
      "'baab': False\n",
      "'bbababb': False\n",
      "'bab': True\n",
      "\n",
      "2.1 (e) - Line starts with integer and ends with a word\n",
      "Matches: True\n",
      "\n",
      "2.1 (f) - String contains both 'grotto' and 'raven'\n",
      "text_f1: True\n",
      "text_f2: False\n",
      "\n",
      "2.1 (g) - Capture first word of a sentence\n",
      "First word: Hello\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"2.1 (a) - All alphabetic strings\")\n",
    "regex_a = r'\\b[a-zA-Z]+\\b'\n",
    "text_a = \"Hello123 this is NLP_lab@2025\"\n",
    "print(\"Matches:\", re.findall(regex_a, text_a))\n",
    "print()\n",
    "\n",
    "print(\"2.1 (b) - Lowercase strings ending in 'b'\")\n",
    "regex_b = r'\\b[a-z]*b\\b'\n",
    "text_b = \"cab grab slab Crib\"\n",
    "print(\"Matches:\", re.findall(regex_b, text_b))\n",
    "print()\n",
    "\n",
    "print(\"2.1 (c) - Two consecutive repeated words\")\n",
    "regex_c = r'\\b(\\w+)\\s+\\1\\b'\n",
    "text_c = \"He said the the word twice, but not like bug bugged him.\"\n",
    "print(\"Matches:\", re.findall(regex_c, text_c))\n",
    "print()\n",
    "\n",
    "print(\"2.1 (d) - Each 'a' is immediately preceded and followed by 'b'\")\n",
    "regex_d = r'^(b|bab)*$'\n",
    "test_strings_d = [\"babbbbab\", \"baab\", \"bbababb\", \"bab\"]\n",
    "for s in test_strings_d:\n",
    "    print(f\"'{s}':\", bool(re.fullmatch(regex_d, s)))\n",
    "print()\n",
    "\n",
    "print(\"2.1 (e) - Line starts with integer and ends with a word\")\n",
    "regex_e = r'^\\d+\\b.*\\b[a-zA-Z]+$'\n",
    "text_e = \"42 the answer is always life\"\n",
    "print(\"Matches:\", bool(re.match(regex_e, text_e)))\n",
    "print()\n",
    "\n",
    "print(\"2.1 (f) - String contains both 'grotto' and 'raven'\")\n",
    "regex_f = r'(?=.*\\bgrotto\\b)(?=.*\\braven\\b)'\n",
    "text_f1 = \"The raven flew above the dark grotto at night.\"\n",
    "text_f2 = \"The grottos were creepy but no raven was seen.\"\n",
    "print(f\"text_f1: {bool(re.search(regex_f, text_f1))}\")\n",
    "print(f\"text_f2: {bool(re.search(regex_f, text_f2))}\")\n",
    "print()\n",
    "\n",
    "print(\"2.1 (g) - Capture first word of a sentence\")\n",
    "regex_g = r'^[\\\"\\'(]*([A-Z][a-z]*)'\n",
    "text_g = '\"Hello there, how are you?\"'\n",
    "match_g = re.match(regex_g, text_g)\n",
    "print(\"First word:\", match_g.group(1) if match_g else \"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe37e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 (a): Single alphabetic words\n",
      "['Hello', 'How', 's', 'treating', 'you', 'dog', 'CAT', 'mouse', 'Mouse', 'fish', 'Paris', 'is', 'in', 'France', 'and', 'Earth', 'is', 'round', 'This', 'test', 'will', 'find', 'four', 'words', 'like', 'done', 'He', 'said', 'the', 'the', 'thing', 'was', 'weird', 'not', 'go', 'go', 'now', 'I', 'was', 'singing', 'and', 'running', 'while', 'eating', 'snacks', 'This', 'book', 'has', 'a', 'letter', 'and', 'a', 'cool', 'story'] \n",
      "\n",
      "2.2 (b): Lowercase words\n",
      "['s', 'treating', 'you', 'dog', 'mouse', 'fish', 'is', 'in', 'and', 'is', 'round', 'test', 'will', 'find', 'four', 'words', 'like', 'done', 'said', 'the', 'the', 'thing', 'was', 'weird', 'not', 'go', 'go', 'now', 'was', 'singing', 'and', 'running', 'while', 'eating', 'snacks', 'book', 'has', 'a', 'letter', 'and', 'a', 'cool', 'story'] \n",
      "\n",
      "2.2 (c): Capitalized words\n",
      "['Hello', 'How', 'Mouse', 'Paris', 'France', 'Earth', 'This', 'He', 'I', 'This'] \n",
      "\n",
      "2.2 (d): Words exactly 4 letters long\n",
      "['fish', 'This', 'test', 'will', 'find', 'four', 'like', 'done', 'said', 'This', 'book', 'cool'] \n",
      "\n",
      "2.2 (e): Repeated consecutive words\n",
      "['the', 'go'] \n",
      "\n",
      "2.2 (f): Words ending in 'ing'\n",
      "['treating', 'thing', 'singing', 'running', 'eating'] \n",
      "\n",
      "2.2 (g): Words with double letters\n",
      "['Hello', 'will', 'running', 'book', 'letter', 'cool']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Hello! How's NLP2025 treating you?\n",
    "dog CAT mouse Mouse fish\n",
    "Paris is in France and Earth is round.\n",
    "This test will find four words like done.\n",
    "He said the the thing was weird, not go go now.\n",
    "I was singing and running while eating snacks.\n",
    "This book has a letter and a cool story.\n",
    "\"\"\"\n",
    "\n",
    "# 2.2 - Regular expressions for word-based patterns\n",
    "\n",
    "# (a) Match a single alphabetic word\n",
    "print(\"2.2 (a): Single alphabetic words\")   \n",
    "print(re.findall(r'\\b[a-zA-Z]+\\b', text), '\\n')\n",
    "\n",
    "# (b) Match only lowercase alphabetic words\n",
    "print(\"2.2 (b): Lowercase words\")\n",
    "print(re.findall(r'\\b[a-z]+\\b', text), '\\n')\n",
    "\n",
    "# (c) Match words starting with a capital letter\n",
    "print(\"2.2 (c): Capitalized words\")\n",
    "print(re.findall(r'\\b[A-Z][a-z]*\\b', text), '\\n')\n",
    "\n",
    "# (d) Match all 4-letter words\n",
    "print(\"2.2 (d): Words exactly 4 letters long\")\n",
    "print(re.findall(r'\\b[a-zA-Z]{4}\\b', text), '\\n')\n",
    "\n",
    "# (e) Match repeated words (like \"go go\")\n",
    "print(\"2.2 (e): Repeated consecutive words\")\n",
    "print(re.findall(r'\\b(\\w+)\\s+\\1\\b', text), '\\n')\n",
    "\n",
    "# (f) Match words ending in 'ing'\n",
    "print(\"2.2 (f): Words ending in 'ing'\")\n",
    "print(re.findall(r'\\b\\w+ing\\b', text), '\\n')\n",
    "\n",
    "# (g) Match words with at least one double letter\n",
    "print(\"2.2 (g): Words with double letters\")\n",
    "matches = re.finditer(r'\\b\\w*(\\w)\\1\\w*\\b', text)\n",
    "print([m.group(0) for m in matches])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
